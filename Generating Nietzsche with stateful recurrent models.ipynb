{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful recurrent models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook *Character-based language model from scratch* we arrived at a point where we had a model that used characters 0 to 7, for example, to predict characters 1 to 8.\n",
    "\n",
    "However, the problem is that when predicting the next set of characters, the hidden state of the rnn is reset to zero. Let's improve on this by preserving the hidden state (while detaching it from its history).\n",
    "\n",
    "Difference to prev notebook: In the previous model the first bs=512 consecutive sequences of bptt=8 characters were part of the first minibatch, etc... When preserving the hidden state, you don't want this. Instead you want to split the entire text into bs=512 equal sized sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab, data\n",
    "\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/nietzsche/'\n",
    "\n",
    "TRN_PATH = 'trn/'\n",
    "VAL_PATH = 'val/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field is a set of instructions on how to process the text\n",
    "# list('abc') gives ['a', 'b', 'c'], so the tokens\n",
    "# are the unique characters\n",
    "\n",
    "TEXT = data.Field(lower=True, tokenize=list) \n",
    "bs=64\n",
    "bptt=8\n",
    "n_emb=42\n",
    "n_hidden=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldata = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(947, 55, 1, 485749)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(modeldata.trn_dl), modeldata.nt, len(modeldata.trn_ds), len(modeldata.trn_ds[0].text)  \n",
    "# number of batches, nt = number of unique tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRnn(nn.Module):\n",
    "    def __init__(self, voc_size, n_emb, bs, nh):\n",
    "        super().__init__()\n",
    "        self.voc_size = voc_size\n",
    "        self.nh = nh\n",
    "        self.e = nn.Embedding(self.voc_size, n_emb)\n",
    "        self.rnn = nn.RNN(n_emb, nh)\n",
    "        self.l_out = nn.Linear(nh, voc_size)\n",
    "        self.init_hidden_state(bs)\n",
    "\n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        # The last minibatch might be smaller\n",
    "        # We need to account for this:\n",
    "        if self.h.size(1) != bs:\n",
    "            self.init_hidden_state(bs)\n",
    "        outp, h = self.rnn(self.e(cs), self.h)\n",
    "        \n",
    "        # Wraps h in new Variables, to detach it \n",
    "        # from its history of operations.\n",
    "        # Backprop will stop here\n",
    "        self.h = repackage_var(h)\n",
    "        \n",
    "        # We need to flatten the output (view)\n",
    "        # because loss functions in pytorch\n",
    "        # currently don't support rank 3 tensors\n",
    "        # (bs, bptt, n_voc)\n",
    "        # target is flattened automatically\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.voc_size)\n",
    "    \n",
    "    \n",
    "    def init_hidden_state(self, bs):\n",
    "        self.h = V(torch.zeros(1, bs, self.nh))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharSeqStatefulRnn(modeldata.nt, n_emb, 512, n_hidden).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193a7ef0a2c24cf7aa518571a9179c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.869997   1.858461  \n",
      "    1      1.689194   1.703229                               \n",
      "    2      1.59974    1.638832                               \n",
      "    3      1.55035    1.597093                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.59709])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, modeldata, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a2d487d2f046d8b0997560ddebb6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.47539    1.557163  \n",
      "    1      1.473396   1.551019                               \n",
      "    2      1.47469    1.547668                               \n",
      "    3      1.464748   1.543166                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.54317])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, modeldata, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look deeper into nn.RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From pytorch source code:\n",
    "\n",
    "```\n",
    "def RNNCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n",
    "   return F.tanh(F.linear(input, w_ih, b_ih) +\n",
    "          F.linear(hidden, w_hh, b_hh))\n",
    "```\n",
    "\n",
    "Interestingly pytorch does not concat but simply add.\n",
    "\n",
    "*Tanh* is often used because it appears to be better at avoiding exploding gradients than *relu*.\n",
    "\n",
    "Obviously do not do it like this by hand unless you want to use a new type of cell or other new concept that does not exist in pytorch yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRnn2(nn.Module):\n",
    "    def __init__(self, voc_size, n_emb, bs, nh):\n",
    "        super().__init__()\n",
    "        self.voc_size = voc_size\n",
    "        self.nh = nh\n",
    "        self.e = nn.Embedding(self.voc_size, n_emb)\n",
    "        self.rnn = nn.RNNCell(n_emb, nh)\n",
    "        self.l_out = nn.Linear(nh, voc_size)\n",
    "        self.init_hidden_state(bs)\n",
    "\n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs:\n",
    "            self.init_hidden_state(bs)\n",
    "        \n",
    "        # To append the outputs\n",
    "        outp = []\n",
    "        o = self.h\n",
    "        for c in cs:\n",
    "            o = self.rnn(self.e(c), o)\n",
    "            outp.append(o)\n",
    "            \n",
    "        outp = self.l_out(torch.stack(outp))\n",
    "        self.h = repackage_var(o)\n",
    "        \n",
    "        return F.log_softmax(outp, dim=-1).view(-1, self.voc_size)\n",
    "    \n",
    "    def init_hidden_state(self, bs):\n",
    "        self.h = V(torch.zeros(1, bs, self.nh))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharSeqStatefulRnn2(modeldata.nt, n_emb, 512, n_hidden).cuda()\n",
    "opt = optim.Adam(model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e52c6e2f77e4763aa73db7c0356873f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.8718     1.85822   \n",
      "    1      1.687939   1.69713                                \n",
      "    2      1.60298    1.628003                               \n",
      "    3      1.549262   1.599414                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.59941])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, modeldata, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nobody uses this rnn cell in practice. Gradient vanishing/exploding is still a problem despite the tanh so you need small learning rates and low values for bptt. Use GRU or LSTM instead.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU (Gated recurrent unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pytorch source:\n",
    "\n",
    "```\n",
    "def GRUCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n",
    "    gi = F.linear(input, w_ih, b_ih)\n",
    "    gh = F.linear(hidden, w_hh, b_hh)\n",
    "    i_r, i_i, i_n = gi.chunk(3, 1)\n",
    "    h_r, h_i, h_n = gh.chunk(3, 1)\n",
    "\n",
    "    resetgate = F.sigmoid(i_r + h_r)\n",
    "    inputgate = F.sigmoid(i_i + h_i)\n",
    "    newgate = F.tanh(i_n + resetgate * h_n)\n",
    "    return newgate + inputgate * (hidden - newgate)\n",
    "```\n",
    "\n",
    "To solve the vanishing gradient problem, GRU uses the so callec **update gate** and **reset gate**.\n",
    "\n",
    "**Update gate:**\n",
    "\n",
    "The update gate determines how much of the previous information needs to be passed to the future.\n",
    "\n",
    "$z_t = \\sigma\\left(W_z\\cdot[h_{t-1}, x_t]\\right)$\n",
    "\n",
    "**Reset Gate:**\n",
    "\n",
    "Determines how much of the past information to forget. The network could for example learn to forget almost everything once it finds a \".\"\n",
    "\n",
    "$r_t = \\sigma\\left(W_r\\cdot[h_{t-1}, x_t]\\right)$\n",
    "\n",
    "\n",
    "**Use the reset gate to calculate the current memory content:**\n",
    "\n",
    "$\\tilde h_t=\\tanh\\left(W\\cdot[r_t\\ast h_{t-1}, xt]\\right)$\n",
    "\n",
    "**Calculate the new hidden state as interpolation of old hidden state and current memory content using the update gate:**\n",
    "\n",
    "$h_t=(1-z_t)\\ast h_{t-1} + z_t\\ast\\tilde h_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulGRU(nn.Module):\n",
    "    def __init__(self, voc_size, n_emb, bs, nh):\n",
    "        super().__init__()\n",
    "        self.voc_size = voc_size\n",
    "        self.nh = nh\n",
    "        self.e = nn.Embedding(self.voc_size, n_emb)\n",
    "        self.rnn = nn.GRU(n_emb, nh)\n",
    "        self.l_out = nn.Linear(nh, voc_size)\n",
    "        self.init_hidden_state(bs)\n",
    "\n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        # The last minibatch might be smaller\n",
    "        # We need to account for this:\n",
    "        if self.h.size(1) != bs:\n",
    "            self.init_hidden_state(bs)\n",
    "        \n",
    "        outp, h = self.rnn(self.e(cs), self.h)\n",
    "        \n",
    "        # Wraps h in new Variables, to detach it \n",
    "        # from its history of operations.\n",
    "        # Backprop will stop here\n",
    "        self.h = repackage_var(h)\n",
    "        \n",
    "        # We need to flatten the output (view)\n",
    "        # because loss functions in pytorch\n",
    "        # currently don't support rank 3 tensors\n",
    "        # (bs, bptt, n_voc)\n",
    "        # target is flattened automatically\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.voc_size)\n",
    "    \n",
    "    \n",
    "    def init_hidden_state(self, bs):\n",
    "        self.h = V(torch.zeros(1, bs, self.nh))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharSeqStatefulGRU(modeldata.nt, n_emb, 512, n_hidden).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccc8754d9074e13abfdcea540f6435d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.743342   1.736539  \n",
      "    1      1.562659   1.581666                               \n",
      "    2      1.477294   1.526515                               \n",
      "    3      1.42534    1.490105                               \n",
      "    4      1.386104   1.468811                               \n",
      "    5      1.356454   1.464969                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.46497])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, modeldata, 6, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6b6e9904e94c349a8f1262841fdb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.271521   1.429878  \n",
      "    1      1.27452    1.426042                               \n",
      "    2      1.267621   1.425261                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.42526])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, modeldata, 3, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "LSTM has an additional *cell state* compared to GRU. You therefore need to initialize an empty *tuple* of hidden states.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import sgdr\n",
    "\n",
    "n_hidden = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulLSTM(nn.Module):\n",
    "    def __init__(self, voc_size, n_emb, bs, nh, nl):\n",
    "        super().__init__()\n",
    "        self.voc_size = voc_size\n",
    "        self.nl = nl\n",
    "        self.nh = nh\n",
    "        self.e = nn.Embedding(self.voc_size, n_emb)\n",
    "        self.rnn = nn.LSTM(n_emb, nh, nl, dropout = 0.5)\n",
    "        self.l_out = nn.Linear(nh, voc_size)\n",
    "        self.init_hidden_state(bs)\n",
    "\n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h[0].size(1) != bs:\n",
    "            self.init_hidden_state(bs)\n",
    "        \n",
    "        outp, h = self.rnn(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        \n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.voc_size)\n",
    "    \n",
    "    def init_hidden_state(self, bs):\n",
    "        self.h = (V(torch.zeros(self.nl, bs, self.nh)),\n",
    "                  V(torch.zeros(self.nl, bs, self.nh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharSeqStatefulLSTM(modeldata.nt, n_emb, 512, n_hidden, 2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo = LayerOptimizer(optim.Adam, model, lrs=1e-2, wds=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{PATH}models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e21d5f31aac4fc3975dc8f2f2e4bfad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.636604   1.592825  \n",
      "    1      1.610442   1.5788                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.5788])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, modeldata, 2, lo.opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc3fbcaa95842279f3fab4688ee2e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.464206   1.454684  \n",
      "    1      1.51953    1.49333                                \n",
      "    2      1.411895   1.416746                               \n",
      "    3      1.550094   1.503142                               \n",
      "    4      1.486141   1.469236                               \n",
      "    5      1.405634   1.411643                               \n",
      "    6      1.350604   1.379368                               \n",
      "    7      1.534413   1.517684                               \n",
      "    8      1.505779   1.487957                               \n",
      "    9      1.477565   1.462818                               \n",
      "    10     1.447093   1.445088                               \n",
      "    11     1.411204   1.415879                               \n",
      "    12     1.368016   1.382721                               \n",
      "    13     1.332156   1.363008                               \n",
      "    14     1.305085   1.350068                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.35007])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_end = lambda sched, cycle: save_model(model, f'{PATH}models/cyc_{cycle}')\n",
    "\n",
    "cb = [CosAnneal(lo, len(modeldata.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "\n",
    "fit(model, modeldata, 2**4 - 1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72145487a1e446c866a5768d523eae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=63), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.292349   1.347956  \n",
      "    1      1.2951     1.347379                               \n",
      "    2      1.29271    1.345982                               \n",
      "    3      1.294296   1.344165                               \n",
      "    4      1.289099   1.341886                               \n",
      "    5      1.281891   1.340308                               \n",
      "    6      1.283003   1.339759                               \n",
      "    7      1.28202    1.340669                               \n",
      "    8      1.2719     1.338476                               \n",
      "    9      1.265462   1.337162                               \n",
      "    10     1.263195   1.335769                               \n",
      "    11     1.260501   1.334463                               \n",
      "    12     1.253951   1.33328                                \n",
      "    13     1.249763   1.332153                               \n",
      "    14     1.251737   1.331728                               \n",
      "    15     1.259258   1.334247                               \n",
      "    16     1.248938   1.332205                               \n",
      "    17     1.245203   1.33346                                \n",
      "    18     1.235381   1.333057                               \n",
      "    19     1.229261   1.332982                               \n",
      "    20     1.224057   1.331426                               \n",
      "    21     1.218922   1.332177                               \n",
      "    22     1.213214   1.33024                                \n",
      "    23     1.202667   1.330456                               \n",
      "    24     1.202109   1.330944                               \n",
      "    25     1.197327   1.331197                               \n",
      "    26     1.192655   1.331555                               \n",
      "    27     1.187927   1.330911                               \n",
      "    28     1.187716   1.330134                               \n",
      "    29     1.182813   1.329683                               \n",
      "    30     1.184329   1.329262                               \n",
      "    31     1.179297   1.329076                               \n",
      "    32     1.195995   1.333232                               \n",
      "    33     1.189994   1.332969                               \n",
      "    34     1.18351    1.334054                               \n",
      "    35     1.175679   1.334477                               \n",
      "    36     1.174624   1.335781                               \n",
      "    37     1.165728   1.338459                               \n",
      "    38     1.159717   1.34057                                \n",
      "    39     1.151335   1.340856                               \n",
      "    40     1.143954   1.341808                               \n",
      "    41     1.134376   1.345014                               \n",
      "    42     1.13303    1.347178                               \n",
      "    43     1.122598   1.348232                               \n",
      "    44     1.11681    1.350042                               \n",
      "    45     1.104158   1.353502                               \n",
      "    46     1.10359    1.355382                               \n",
      "    47     1.090719   1.356922                               \n",
      "    48     1.093754   1.356853                               \n",
      "    49     1.082198   1.359674                               \n",
      "    50     1.07469    1.361513                               \n",
      "    51     1.070811   1.363592                               \n",
      "    52     1.064376   1.364402                               \n",
      "    53     1.067881   1.364574                               \n",
      "    54     1.063486   1.364874                               \n",
      "    55     1.062783   1.365408                               \n",
      "    56     1.050773   1.365057                               \n",
      "    57     1.052888   1.366021                               \n",
      "    58     1.055146   1.36704                                \n",
      "    59     1.049111   1.366463                               \n",
      "    60     1.054857   1.366284                               \n",
      "    61     1.050695   1.366865                               \n",
      "    62     1.046007   1.366425                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.36642])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_end = lambda sched, cycle: save_model(model, f'{PATH}models/cyc_{cycle}')\n",
    "\n",
    "cb = [CosAnneal(lo, len(modeldata.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "\n",
    "fit(model, modeldata, 2**6 - 1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = model(VV(idxs.transpose(0, 1)))\n",
    "    r = torch.multinomial(p[-1].exp(), 1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    result = inp\n",
    "    for i in range(n):\n",
    "        char = get_next(inp)\n",
    "        result += char\n",
    "        inp = inp[1:] + char\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for those exclusion of the root of through their anth--life as yet grow theleops, again, difficulty, divined claal man, weel viced agrown,diffule, trained, and afwords of history of this all godand depth, to overlooks or to other. for this hand. how possiblity! so that one must necessarily responsible, sequently fredom!\" or oven our culture to be expediency, instinct, rationary evidence, again philosophy--\n"
     ]
    }
   ],
   "source": [
    "print(get_next_n('for those', 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok, Nietzsche does sound a little better :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning_save_2]",
   "language": "python",
   "name": "conda-env-deeplearning_save_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
